#!/usr/bin/env python3
import math
import statistics as stats
import torch
import triton
import triton.language as tl
import torch.nn.functional as F

# -----------------------------
# Triton kernel (GEGLU in rank space)
# -----------------------------
@triton.jit
def fused_ffn_phase1_geglu(
    P_ptr, V1_ptr, U2_ptr, S_ptr, b1_ptr,
    B, L, D, R1, R2,
    sP_b, sP_l, sP_r1,
    sV1_r1, sV1_d,
    sU2_d, sU2_r2,
    sb1,
    sS_b, sS_l, sS_r2,
    BL: tl.constexpr, BD: tl.constexpr, BR1: tl.constexpr, BR2: tl.constexpr,
    USE_TANH: tl.constexpr,
):
    pid_b  = tl.program_id(0)
    pid_l  = tl.program_id(1)
    pid_r2 = tl.program_id(2)

    offs_l  = pid_l * BL + tl.arange(0, BL)
    offs_r2 = pid_r2 * BR2 + tl.arange(0, BR2)

    Tu_acc = tl.zeros((BL, BD), dtype=tl.float32)
    Tv_acc = tl.zeros((BL, BD), dtype=tl.float32)
    acc    = tl.zeros((BL, BR2), dtype=tl.float32)

    c0 = 0.7978845608028654  # sqrt(2/pi)
    c1 = 0.044715
    inv_sqrt2 = 0.7071067811865476

    for d0 in range(0, D, BD):
        d   = d0 + tl.arange(0, BD)
        m_d = d < D

        Tu_acc *= 0.0
        Tv_acc *= 0.0

        for r1_0 in range(0, R1, BR1):
            r1   = r1_0 + tl.arange(0, BR1)
            m_r1 = r1 < R1

            P_blk = tl.load(
                P_ptr + pid_b * sP_b + offs_l[:, None]*sP_l + r1[None, :]*sP_r1,
                mask=(offs_l[:, None] < L) & m_r1[None, :],
                other=0.0
            )

            V1u_blk = tl.load(
                V1_ptr + r1[:, None]*sV1_r1 + d[None, :]*sV1_d,
                mask=m_r1[:, None] & m_d[None, :],
                other=0.0
            )

            V1v_blk = tl.load(
                V1_ptr + r1[:, None]*sV1_r1 + (d[None, :] + D)*sV1_d,
                mask=m_r1[:, None] & m_d[None, :],
                other=0.0
            )

            Tu_acc += tl.dot(P_blk, V1u_blk)
            Tv_acc += tl.dot(P_blk, V1v_blk)

        b1u = tl.load(b1_ptr + d*sb1,        mask=m_d, other=0.0).to(tl.float32)
        b1v = tl.load(b1_ptr + (d + D)*sb1,  mask=m_d, other=0.0).to(tl.float32)
        Tu  = Tu_acc + b1u[None, :]
        Tv  = Tv_acc + b1v[None, :]

        if USE_TANH:
            z  = c0 * (Tu + c1 * Tu * Tu * Tu)
            z2 = 2.0 * z
            sig_2z = tl.where(
                z2 >= 0,
                1.0 / (1.0 + tl.exp(-z2)),
                tl.exp(z2) / (1.0 + tl.exp(z2))
            )
            tanh_z = 2.0 * sig_2z - 1.0
            Hu = 0.5 * Tu * (1.0 + tanh_z)
        else:
            Hu = 0.5 * Tu * (1.0 + tl.erf(Tu * inv_sqrt2))

        H = Hu * Tv

        U2_blk = tl.load(
            U2_ptr + d[:, None]*sU2_d + offs_r2[None, :]*sU2_r2,
            mask=m_d[:, None] & (offs_r2[None, :] < R2),
            other=0.0
        ).to(tl.float32)
        acc += tl.dot(H, U2_blk)

    mask = (offs_l[:, None] < L) & (offs_r2[None, :] < R2)
    tl.store(
        S_ptr + pid_b*sS_b + offs_l[:, None]*sS_l + offs_r2[None, :]*sS_r2,
        acc, mask=mask
    )

# -----------------------------
# Wrappers
# -----------------------------
def flashsvd_ffn_geglu(
    P, V1, U2, V2, b1, b2,
    BL=64, BD=64, BR1=64, BR2=64,
    gelu_approx: str = "tanh",
    nan_scrub: bool = False,
    store_s_fp32: bool = False,
):
    assert P.is_cuda and V1.is_cuda and U2.is_cuda and V2.is_cuda
    B, L, R1 = P.shape
    R1_v1, twoD = V1.shape
    D = twoD // 2
    assert R1_v1 == R1 and twoD == 2 * D
    D_u2, R2 = U2.shape
    assert D_u2 == D
    R2_v2, H = V2.shape
    assert R2_v2 == R2
    assert b1.shape[0] == 2*D and b2.shape[0] == H

    S_dtype = torch.float32 if store_s_fp32 else P.dtype
    S = torch.empty((B, L, R2), device=P.device, dtype=S_dtype)

    strides = dict(
        sP_b=P.stride(0), sP_l=P.stride(1), sP_r1=P.stride(2),
        sV1_r1=V1.stride(0), sV1_d=V1.stride(1),
        sU2_d=U2.stride(0), sU2_r2=U2.stride(1),
        sb1=b1.stride(0),
        sS_b=S.stride(0), sS_l=S.stride(1), sS_r2=S.stride(2),
    )

    grid = (B, triton.cdiv(L, BL), triton.cdiv(R2, BR2))
    USE_TANH = 1 if gelu_approx == "tanh" else 0

    fused_ffn_phase1_geglu[grid](
        P, V1, U2, S, b1,
        B, L, D, R1, R2,
        *strides.values(),
        BL, BD, BR1, BR2,
        USE_TANH=USE_TANH,
    )

    if nan_scrub:
        S = torch.nan_to_num(S, nan=0.0, posinf=0.0, neginf=0.0)

    Y = S.matmul(V2)
    Y = Y + b2.view(1, 1, -1)
    return Y

def _pt_baseline(P, V1, U2, V2, b1, b2, gelu_approx="tanh"):
    Z  = P.matmul(V1) + b1.view(1, 1, -1)
    Zu, Zv = Z.split(Z.shape[-1] // 2, dim=-1)
    H  = F.gelu(Zu, approximate=gelu_approx) * Zv
    S  = H.matmul(U2)
    Y  = S.matmul(V2) + b2.view(1, 1, -1)
    return Y

# -----------------------------
# Memory estimators (activations only)
# -----------------------------
def mib(nbytes): return nbytes / (1024**2)

def theoretical_peak_bytes_baseline(B, L, H, D, R2, dtype):
    bytes_e = torch.tensor([], dtype=dtype).element_size()
    Z = bytes_e * B * L * (2*D)
    Ht = bytes_e * B * L * D
    S = bytes_e * B * L * R2
    Y = bytes_e * B * L * H
    peak_H_stage = Z + Ht
    peak_Y_stage = S + Y
    peak_S_stage = Ht + S
    return max(peak_H_stage, peak_S_stage, peak_Y_stage, Z)

def theoretical_peak_bytes_triton(B, L, H, R2, dtype, store_s_fp32=False):
    el_S = torch.tensor([], dtype=torch.float32 if store_s_fp32 else dtype).element_size()
    el_Y = torch.tensor([], dtype=dtype).element_size()
    S = el_S * B * L * R2
    Y = el_Y * B * L * H
    return S + Y

# -----------------------------
# Timing helper
# -----------------------------
@torch.no_grad()
def bench(fn, n_warmup=10, n_runs=50):
    # CUDA event-based timing
    start = torch.cuda.Event(enable_timing=True)
    end   = torch.cuda.Event(enable_timing=True)
    times_ms = []

    # warmup
    for _ in range(n_warmup):
        _ = fn()
    torch.cuda.synchronize()

    # timed runs
    for _ in range(n_runs):
        start.record()
        out = fn()
        end.record()
        torch.cuda.synchronize()
        times_ms.append(start.elapsed_time(end))
        # tiny usage to keep out alive
        _ = out.view(-1)[0].item()

    return {
        "mean_ms": sum(times_ms)/len(times_ms),
        "median_ms": stats.median(times_ms),
        "p95_ms": stats.quantiles(times_ms, n=20)[-1] if len(times_ms) >= 20 else max(times_ms),
        "all_ms": times_ms,
    }

# -----------------------------
# Main
# -----------------------------
def main():
    torch.backends.cuda.matmul.allow_tf32 = True
    device = torch.device("cuda")
    torch.manual_seed(0)

    # dims
    B, L, H = 8, 2048, 768
    D       = 3072
    R1, R2  = 384, 384
    BL, BD, BR1, BR2 = 64, 64, 64, 64

    dtype = torch.float16
    X  = torch.randn((B, L, H), device=device, dtype=dtype)

    U1 = torch.randn((H, R1), device=device, dtype=dtype) / math.sqrt(H)
    V1 = torch.randn((R1, 2*D), device=device, dtype=dtype) / math.sqrt(R1)
    U2 = torch.randn((D,  R2), device=device, dtype=dtype) / math.sqrt(D)
    V2 = torch.randn((R2, H),  device=device, dtype=dtype) / math.sqrt(R2)
    b1 = torch.zeros((2*D,), device=device, dtype=dtype)
    b2 = torch.zeros((H,),   device=device, dtype=dtype)

    # rank-space input
    P  = X.matmul(U1)

    # warmup Triton once (compile)
    _ = flashsvd_ffn_geglu(P, V1, U2, V2, b1, b2, BL, BD, BR1, BR2,
                           gelu_approx="tanh", nan_scrub=False, store_s_fp32=False)
    torch.cuda.synchronize()

    # measured peak memory
    torch.cuda.reset_peak_memory_stats(device)
    out_triton = flashsvd_ffn_geglu(P, V1, U2, V2, b1, b2, BL, BD, BR1, BR2,
                                    gelu_approx="tanh", nan_scrub=False, store_s_fp32=False)
    torch.cuda.synchronize()
    peak_triton = torch.cuda.max_memory_allocated(device)

    torch.cuda.reset_peak_memory_stats(device)
    out_pt = _pt_baseline(P, V1, U2, V2, b1, b2, gelu_approx="tanh")
    torch.cuda.synchronize()
    peak_baseline = torch.cuda.max_memory_allocated(device)

    # correctness
    diff  = (out_triton - out_pt).to(torch.float32)
    rel_f = diff.norm() / (out_pt.to(torch.float32).norm() + 1e-12)
    print("finite(triton):", torch.isfinite(out_triton).all().item(),
          " finite(pt):", torch.isfinite(out_pt).all().item())
    print("finite(diff):", torch.isfinite(diff).all().item())
    print(f"Max abs error: {torch.nan_to_num(diff.abs().max(), nan=0.0).item():.3e}")
    print(f"Rel Fro error: {rel_f.item():.3e}")

    # theoretical activation peaks
    theo_base = theoretical_peak_bytes_baseline(B, L, H, D, R2, dtype)
    theo_trit = theoretical_peak_bytes_triton(B, L, H, R2, dtype, store_s_fp32=False)

    print("\n=== Peak Memory (measured) ===")
    print(f"Baseline (PyTorch): {mib(peak_baseline):,.2f} MiB")
    print(f"FlashSVD (Triton): {mib(peak_triton):,.2f} MiB")

    print("\n=== Peak Memory (theoretical activations) ===")
    print(f"Baseline (PyTorch): {mib(theo_base):,.2f} MiB")
    print(f"FlashSVD (Triton): {mib(theo_trit):,.2f} MiB")

    saved_meas = peak_baseline - peak_triton
    saved_theo = theo_base - theo_trit
    print("\n=== Savings ===")
    print(f"Measured savings: {mib(max(saved_meas, 0)):,.2f} MiB")
    print(f"Theoretical savings: {mib(max(saved_theo, 0)):,.2f} MiB")
    if theo_base > 0:
        print("Theoretical ratio (FlashSVD / Baseline):",
              f"{(theo_trit / theo_base):.3f}x")

    # ------------------ SPEED: latency + tokens/sec ------------------
    tokens = B * L
    # Benchmark functions capture current tensors by closure
    triton_stats = bench(lambda: flashsvd_ffn_geglu(P, V1, U2, V2, b1, b2,
                                                    BL, BD, BR1, BR2,
                                                    gelu_approx="tanh",
                                                    nan_scrub=False, store_s_fp32=False),
                         n_warmup=10, n_runs=50)
    base_stats   = bench(lambda: _pt_baseline(P, V1, U2, V2, b1, b2, gelu_approx="tanh"),
                         n_warmup=10, n_runs=50)

    def tps(ms):  # tokens per second
        return tokens / (ms / 1e3)

    print("\n=== Inference Speed (latency per forward) ===")
    print("Baseline (PyTorch): "
          f"mean {base_stats['mean_ms']:.2f} ms | median {base_stats['median_ms']:.2f} ms | p95 {base_stats['p95_ms']:.2f} ms "
          f"| {tps(base_stats['mean_ms']):,.0f} tok/s")
    print("FlashSVD (Triton): "
          f"mean {triton_stats['mean_ms']:.2f} ms | median {triton_stats['median_ms']:.2f} ms | p95 {triton_stats['p95_ms']:.2f} ms "
          f"| {tps(triton_stats['mean_ms']):,.0f} tok/s")

if __name__ == "__main__":
    main()






























# #!/usr/bin/env python3
# import math
# import torch
# import triton
# import triton.language as tl
# import torch.nn.functional as F

# # -----------------------------------------------------------------------------
# # Optional NaN scrub (OFF by default). Not an approximation of GELU—just replaces
# # NaNs that may come from earlier ops / overflow.
# # -----------------------------------------------------------------------------
# @triton.jit
# def _nan_scrub(x, enable: tl.constexpr):
#     return tl.where(x == x, x, 0.0) if enable else x  # NaN != NaN

# # -----------------------------------------------------------------------------
# # Kernel: S = (GEGLU(P @ V1 + b1)) @ U2
# # - Exact(erf) GELU ONLY (no tanh path, no clamping)
# # - All matmul math in fp32
# # -----------------------------------------------------------------------------
# @triton.jit
# def fused_ffn_phase1_geglu_erf(
#     P_ptr, V1_ptr, U2_ptr, S_ptr, b1_ptr,
#     B, L, D, R1, R2,
#     sP_b, sP_l, sP_r1,
#     sV1_r1, sV1_d,
#     sU2_d, sU2_r2,
#     sb1,
#     sS_b, sS_l, sS_r2,
#     BL: tl.constexpr, BD: tl.constexpr, BR1: tl.constexpr, BR2: tl.constexpr,
#     NAN_SCRUB: tl.constexpr,   # 0 or 1; purity -> set 0
# ):
#     pid_b  = tl.program_id(0)
#     pid_l  = tl.program_id(1)
#     pid_r2 = tl.program_id(2)

#     offs_l  = pid_l * BL + tl.arange(0, BL)
#     offs_r2 = pid_r2 * BR2 + tl.arange(0, BR2)

#     # fp32 accumulators
#     Tu_acc = tl.zeros((BL, BD), dtype=tl.float32)
#     Tv_acc = tl.zeros((BL, BD), dtype=tl.float32)
#     acc    = tl.zeros((BL, BR2), dtype=tl.float32)

#     inv_sqrt2 = 0.7071067811865476  # 1/sqrt(2)

#     # tile over D
#     for d0 in range(0, D, BD):
#         d   = d0 + tl.arange(0, BD)
#         m_d = d < D

#         # reset per-D tile
#         Tu_acc = tl.zeros((BL, BD), dtype=tl.float32)
#         Tv_acc = tl.zeros((BL, BD), dtype=tl.float32)

#         # accumulate over R1
#         for r1_0 in range(0, R1, BR1):
#             r1   = r1_0 + tl.arange(0, BR1)
#             m_r1 = r1 < R1

#             P_blk = tl.load(
#                 P_ptr + pid_b * sP_b + offs_l[:, None] * sP_l + r1[None, :] * sP_r1,
#                 mask=(offs_l[:, None] < L) & m_r1[None, :],
#                 other=0.0
#             ).to(tl.float32)

#             V1u_blk = tl.load(
#                 V1_ptr + r1[:, None] * sV1_r1 + d[None, :] * sV1_d,
#                 mask=m_r1[:, None] & m_d[None, :],
#                 other=0.0
#             ).to(tl.float32)

#             V1v_blk = tl.load(
#                 V1_ptr + r1[:, None] * sV1_r1 + (d[None, :] + D) * sV1_d,
#                 mask=m_r1[:, None] & m_d[None, :],
#                 other=0.0
#             ).to(tl.float32)

#             if NAN_SCRUB:
#                 P_blk   = _nan_scrub(P_blk, 1)
#                 V1u_blk = _nan_scrub(V1u_blk, 1)
#                 V1v_blk = _nan_scrub(V1v_blk, 1)

#             Tu_acc += tl.dot(P_blk, V1u_blk)  # [BL, BD]
#             Tv_acc += tl.dot(P_blk, V1v_blk)  # [BL, BD]

#         # bias add (fp32)
#         b1u = tl.load(b1_ptr + d * sb1,       mask=m_d, other=0.0).to(tl.float32)
#         b1v = tl.load(b1_ptr + (d + D) * sb1, mask=m_d, other=0.0).to(tl.float32)
#         Tu  = Tu_acc + b1u[None, :]
#         Tv  = Tv_acc + b1v[None, :]

#         if NAN_SCRUB:
#             Tu = _nan_scrub(Tu, 1)
#             Tv = _nan_scrub(Tv, 1)

#         # --- GELU ---
#         Hu = 0.5 * Tu * (1.0 + tl.erf(Tu * inv_sqrt2))  # [BL, BD] fp32
#         H  = Hu * Tv                                    # [BL, BD] fp32

#         if NAN_SCRUB:
#             H = _nan_scrub(H, 1)

#         # multiply by U2 tile
#         U2_blk = tl.load(
#             U2_ptr + d[:, None] * sU2_d + offs_r2[None, :] * sU2_r2,
#             mask=m_d[:, None] & (offs_r2[None, :] < R2),
#             other=0.0
#         ).to(tl.float32)
#         if NAN_SCRUB:
#             U2_blk = _nan_scrub(U2_blk, 1)

#         acc += tl.dot(H, U2_blk)  # [BL, BR2]

#     # store (Triton will cast to S dtype)
#     mask = (offs_l[:, None] < L) & (offs_r2[None, :] < R2)
#     tl.store(
#         S_ptr + pid_b * sS_b + offs_l[:, None] * sS_l + offs_r2[None, :] * sS_r2,
#         acc, mask=mask
#     )

# # -----------------------------------------------------------------------------
# # Python wrapper
# # -----------------------------------------------------------------------------
# def flashsvd_ffn_geglu(
#     P, V1, U2, V2, b1, b2,
#     BL=64, BD=128, BR1=64, BR2=64,
#     nan_scrub: bool = False,     # keep False for the "pure" run
#     store_s_fp32: bool = False,  # try True once to rule out fp16 store overflow
# ):
#     assert P.is_cuda and V1.is_cuda and U2.is_cuda and V2.is_cuda
#     B, L, R1 = P.shape
#     R1_v1, twoD = V1.shape
#     D = twoD // 2
#     assert R1_v1 == R1 and twoD == 2 * D
#     D_u2, R2 = U2.shape
#     assert D_u2 == D
#     R2_v2, H = V2.shape
#     assert R2_v2 == R2
#     assert b1.shape[0] == 2 * D and b2.shape[0] == H

#     S_dtype = torch.float32 if store_s_fp32 else P.dtype
#     S = torch.empty((B, L, R2), device=P.device, dtype=S_dtype)

#     strides = dict(
#         sP_b=P.stride(0), sP_l=P.stride(1), sP_r1=P.stride(2),
#         sV1_r1=V1.stride(0), sV1_d=V1.stride(1),
#         sU2_d=U2.stride(0), sU2_r2=U2.stride(1),
#         sb1=b1.stride(0),
#         sS_b=S.stride(0), sS_l=S.stride(1), sS_r2=S.stride(2),
#     )

#     grid = (B, triton.cdiv(L, BL), triton.cdiv(R2, BR2))
#     fused_ffn_phase1_geglu_erf[grid](
#         P, V1, U2, S, b1,
#         B, L, D, R1, R2,
#         *strides.values(),
#         BL, BD, BR1, BR2,
#         NAN_SCRUB=1 if nan_scrub else 0,
#     )

#     Y = S.matmul(V2) + b2.view(1, 1, -1)
#     return Y

# # -----------------------------------------------------------------------------
# # PyTorch reference
# # -----------------------------------------------------------------------------
# def _pt_baseline(P, V1, U2, V2, b1, b2):
#     Z = P.matmul(V1) + b1.view(1, 1, -1)
#     Zu, Zv = Z.split(Z.shape[-1] // 2, dim=-1)
#     H  = F.gelu(Zu, approximate="none") * Zv
#     S  = H.matmul(U2)
#     Y  = S.matmul(V2) + b2.view(1, 1, -1)
#     return Y

# def main():
#     device = torch.device("cuda")
#     torch.manual_seed(0)

#     B, L, H = 8, 2048, 768
#     D       = 3072
#     R1, R2  = 512, 512
#     BL, BD, BR1, BR2 = 64, 128, 64, 64

#     dtype = torch.float16
#     X  = torch.randn((B, L, H), device=device, dtype=dtype)

#     U1 = torch.randn((H, R1), device=device, dtype=dtype) / math.sqrt(H)
#     V1 = torch.randn((R1, 2*D), device=device, dtype=dtype) / math.sqrt(R1)
#     U2 = torch.randn((D,  R2), device=device, dtype=dtype) / math.sqrt(D)
#     V2 = torch.randn((R2, H),  device=device, dtype=dtype) / math.sqrt(R2)
#     b1 = torch.zeros((2*D,), device=device, dtype=dtype)
#     b2 = torch.zeros((H,),   device=device, dtype=dtype)

#     P  = X.matmul(U1)

#     # Warmup
#     _ = flashsvd_ffn_geglu(P, V1, U2, V2, b1, b2,
#                            BL, BD, BR1, BR2,
#                            nan_scrub=False, store_s_fp32=False)
#     torch.cuda.synchronize()

#     out_triton = flashsvd_ffn_geglu(P, V1, U2, V2, b1, b2,
#                                     BL, BD, BR1, BR2,
#                                     nan_scrub=False, store_s_fp32=False)
#     out_pt     = _pt_baseline(P, V1, U2, V2, b1, b2)

#     print("finite(triton):", torch.isfinite(out_triton).all().item(),
#           " finite(pt):", torch.isfinite(out_pt).all().item())

#     diff  = (out_triton - out_pt).to(torch.float32)
#     print("finite(diff):", torch.isfinite(diff).all().item())
#     rel_f = diff.norm() / (out_pt.to(torch.float32).norm() + 1e-12)
#     print(f"Max abs error: {torch.nan_to_num(diff.abs().max(), nan=0.0).item():.3e}")
#     print(f"Rel Fro error: {rel_f.item():.3e}")

# if __name__ == "__main__":
#     main()
